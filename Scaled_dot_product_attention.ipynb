{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKsg1BDBs8O31kFOE8j0yp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackwithwhitegreen/Transformer/blob/main/Scaled_dot_product_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEutlvhKIZad",
        "outputId": "a7135655-8566-4098-af2b-fcfb03ca5efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Output: tensor([[[[0.6575, 0.4717, 0.6834, 0.4144, 0.4921, 0.5801, 0.4806, 0.6286],\n",
            "          [0.6395, 0.5116, 0.6673, 0.3996, 0.5182, 0.5957, 0.5000, 0.6414],\n",
            "          [0.6393, 0.5015, 0.6601, 0.4059, 0.5172, 0.6047, 0.5085, 0.6425],\n",
            "          [0.6362, 0.5159, 0.6602, 0.4004, 0.5258, 0.5979, 0.5063, 0.6440],\n",
            "          [0.6536, 0.5078, 0.6739, 0.4173, 0.5130, 0.5761, 0.5193, 0.6266]],\n",
            "\n",
            "         [[0.1816, 0.3636, 0.4597, 0.5507, 0.5844, 0.5570, 0.6119, 0.4455],\n",
            "          [0.1843, 0.3706, 0.4432, 0.5678, 0.5881, 0.5430, 0.5984, 0.4326],\n",
            "          [0.1797, 0.3676, 0.4445, 0.5620, 0.5730, 0.5429, 0.5983, 0.4416],\n",
            "          [0.1716, 0.3562, 0.4660, 0.5344, 0.5626, 0.5581, 0.6177, 0.4634],\n",
            "          [0.1823, 0.3715, 0.4456, 0.5656, 0.5704, 0.5465, 0.5972, 0.4411]],\n",
            "\n",
            "         [[0.5588, 0.4885, 0.3971, 0.4725, 0.3751, 0.6071, 0.6803, 0.4354],\n",
            "          [0.5640, 0.4768, 0.3827, 0.4844, 0.3849, 0.6001, 0.6903, 0.4322],\n",
            "          [0.5638, 0.4759, 0.3867, 0.4712, 0.3845, 0.5974, 0.6837, 0.4349],\n",
            "          [0.5598, 0.4909, 0.3876, 0.4972, 0.3772, 0.6112, 0.6936, 0.4300],\n",
            "          [0.5630, 0.4723, 0.3909, 0.4656, 0.3845, 0.5960, 0.6788, 0.4359]],\n",
            "\n",
            "         [[0.4546, 0.5655, 0.5809, 0.4318, 0.4149, 0.5027, 0.4749, 0.5932],\n",
            "          [0.4573, 0.5674, 0.5965, 0.4161, 0.4191, 0.4992, 0.4893, 0.5997],\n",
            "          [0.4554, 0.5492, 0.5715, 0.4391, 0.3965, 0.5182, 0.4748, 0.5740],\n",
            "          [0.4513, 0.5538, 0.5731, 0.4280, 0.4053, 0.5104, 0.4821, 0.5765],\n",
            "          [0.4471, 0.5663, 0.5805, 0.4264, 0.4174, 0.5028, 0.4841, 0.5898]]],\n",
            "\n",
            "\n",
            "        [[[0.5488, 0.5575, 0.3671, 0.3733, 0.6248, 0.3999, 0.3777, 0.4909],\n",
            "          [0.5560, 0.5571, 0.3880, 0.3743, 0.6318, 0.4032, 0.3704, 0.5273],\n",
            "          [0.5531, 0.5367, 0.3666, 0.3567, 0.6151, 0.4177, 0.3553, 0.5265],\n",
            "          [0.5580, 0.5800, 0.3767, 0.3783, 0.6453, 0.3779, 0.4017, 0.4811],\n",
            "          [0.5633, 0.5581, 0.3793, 0.3597, 0.6358, 0.3970, 0.3774, 0.5218]],\n",
            "\n",
            "         [[0.5540, 0.4721, 0.3339, 0.6712, 0.1872, 0.5037, 0.3048, 0.3482],\n",
            "          [0.5781, 0.4641, 0.3096, 0.6647, 0.1988, 0.5179, 0.2917, 0.3394],\n",
            "          [0.5581, 0.4821, 0.3223, 0.6745, 0.2046, 0.5328, 0.2936, 0.3465],\n",
            "          [0.5784, 0.4567, 0.3151, 0.6618, 0.1880, 0.4982, 0.2975, 0.3397],\n",
            "          [0.5764, 0.4694, 0.3266, 0.6681, 0.1881, 0.4904, 0.2912, 0.3429]],\n",
            "\n",
            "         [[0.4428, 0.5508, 0.7892, 0.5577, 0.3313, 0.6582, 0.4709, 0.5396],\n",
            "          [0.4500, 0.5502, 0.7904, 0.5461, 0.3302, 0.6675, 0.4759, 0.5257],\n",
            "          [0.4461, 0.5414, 0.7905, 0.5520, 0.3322, 0.6662, 0.4707, 0.5269],\n",
            "          [0.4466, 0.5472, 0.7914, 0.5562, 0.3336, 0.6589, 0.4660, 0.5317],\n",
            "          [0.4266, 0.5475, 0.7900, 0.5406, 0.3549, 0.6645, 0.4803, 0.5353]],\n",
            "\n",
            "         [[0.5611, 0.7802, 0.4243, 0.5969, 0.3643, 0.4966, 0.6438, 0.4665],\n",
            "          [0.5590, 0.7798, 0.4235, 0.5975, 0.3621, 0.5063, 0.6539, 0.4730],\n",
            "          [0.5659, 0.7838, 0.4378, 0.5987, 0.3606, 0.5049, 0.6548, 0.4591],\n",
            "          [0.5456, 0.7758, 0.3925, 0.5892, 0.3543, 0.5012, 0.6487, 0.5024],\n",
            "          [0.5711, 0.7791, 0.4185, 0.6019, 0.3496, 0.5128, 0.6572, 0.4827]]]])\n",
            "Attention Weights: tensor([[[[0.2849, 0.1770, 0.1937, 0.2004, 0.1440],\n",
            "          [0.2416, 0.1613, 0.2535, 0.1872, 0.1563],\n",
            "          [0.2451, 0.1916, 0.2444, 0.1511, 0.1678],\n",
            "          [0.2293, 0.1702, 0.2626, 0.1773, 0.1605],\n",
            "          [0.2306, 0.1797, 0.2267, 0.1696, 0.1935]],\n",
            "\n",
            "         [[0.2012, 0.1726, 0.2250, 0.2047, 0.1966],\n",
            "          [0.1811, 0.1836, 0.2288, 0.1918, 0.2148],\n",
            "          [0.2004, 0.1867, 0.2182, 0.1803, 0.2144],\n",
            "          [0.2378, 0.1813, 0.2047, 0.1895, 0.1866],\n",
            "          [0.1954, 0.1733, 0.2187, 0.1887, 0.2240]],\n",
            "\n",
            "         [[0.1875, 0.1989, 0.2160, 0.2118, 0.1858],\n",
            "          [0.1861, 0.1875, 0.2463, 0.1880, 0.1921],\n",
            "          [0.1775, 0.1855, 0.2357, 0.1994, 0.2018],\n",
            "          [0.2006, 0.2078, 0.2353, 0.1904, 0.1659],\n",
            "          [0.1738, 0.1782, 0.2358, 0.2057, 0.2064]],\n",
            "\n",
            "         [[0.2062, 0.2338, 0.2037, 0.1665, 0.1898],\n",
            "          [0.1981, 0.2597, 0.1890, 0.1803, 0.1728],\n",
            "          [0.2293, 0.2104, 0.2185, 0.1571, 0.1848],\n",
            "          [0.2272, 0.2191, 0.2016, 0.1623, 0.1898],\n",
            "          [0.2154, 0.2391, 0.1955, 0.1612, 0.1889]]],\n",
            "\n",
            "\n",
            "        [[[0.1636, 0.1727, 0.2703, 0.1910, 0.2025],\n",
            "          [0.1819, 0.1841, 0.2157, 0.1989, 0.2194],\n",
            "          [0.1811, 0.1428, 0.2274, 0.2275, 0.2212],\n",
            "          [0.1557, 0.2205, 0.2693, 0.1799, 0.1747],\n",
            "          [0.1774, 0.1904, 0.2201, 0.2184, 0.1936]],\n",
            "\n",
            "         [[0.2486, 0.1831, 0.1927, 0.1812, 0.1945],\n",
            "          [0.2178, 0.1742, 0.1625, 0.2359, 0.2096],\n",
            "          [0.2409, 0.1717, 0.1686, 0.2408, 0.1780],\n",
            "          [0.2183, 0.1828, 0.1763, 0.2007, 0.2219],\n",
            "          [0.2100, 0.2110, 0.1841, 0.2007, 0.1941]],\n",
            "\n",
            "         [[0.2061, 0.1593, 0.2194, 0.1854, 0.2298],\n",
            "          [0.1989, 0.1614, 0.2180, 0.1759, 0.2458],\n",
            "          [0.1958, 0.1820, 0.2311, 0.1656, 0.2256],\n",
            "          [0.2065, 0.1635, 0.2281, 0.1720, 0.2299],\n",
            "          [0.2219, 0.1723, 0.1938, 0.1663, 0.2456]],\n",
            "\n",
            "         [[0.2140, 0.2034, 0.2206, 0.1660, 0.1960],\n",
            "          [0.2210, 0.2136, 0.2001, 0.1654, 0.1999],\n",
            "          [0.2073, 0.2287, 0.2097, 0.1576, 0.1968],\n",
            "          [0.2234, 0.1847, 0.1819, 0.1939, 0.2161],\n",
            "          [0.2009, 0.2114, 0.1896, 0.1637, 0.2345]]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Computes the Scaled Dot-Product Attention.\n",
        "\n",
        "    Args:\n",
        "    Q: Query matrix of shape (batch_size, num_heads, seq_len, d_k)\n",
        "    K: Key matrix of shape (batch_size, num_heads, seq_len, d_k)\n",
        "    V: Value matrix of shape (batch_size, num_heads, seq_len, d_v)\n",
        "    mask: Optional mask tensor to prevent attending to certain positions.\n",
        "\n",
        "    Returns:\n",
        "    attention_output: The output after applying attention.\n",
        "    attention_weights: The computed attention weights.\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]  # Get the dimension of keys\n",
        "\n",
        "    # Compute attention scores\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "    # Apply mask if provided\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    # Compute attention weights using softmax\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # Compute the final output\n",
        "    attention_output = torch.matmul(attention_weights, V)\n",
        "\n",
        "    return attention_output, attention_weights\n",
        "\n",
        "# Example usage\n",
        "batch_size = 2\n",
        "num_heads = 4\n",
        "seq_len = 5\n",
        "d_k = d_v = 8  # Dimension of key, query, and value vectors\n",
        "\n",
        "# Create random Q, K, V matrices\n",
        "Q = torch.rand(batch_size, num_heads, seq_len, d_k)\n",
        "K = torch.rand(batch_size, num_heads, seq_len, d_k)\n",
        "V = torch.rand(batch_size, num_heads, seq_len, d_v)\n",
        "\n",
        "# Compute attention\n",
        "output, weights = scaled_dot_product_attention(Q, K, V)\n",
        "print(\"Attention Output:\", output)\n",
        "print(\"Attention Weights:\", weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aTf_byb2IbJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}