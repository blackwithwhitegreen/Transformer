{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD74dMNKDt7cdmkVUWleUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackwithwhitegreen/Transformer/blob/main/Scaled_dot_product_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6p5GEM2aJuZ",
        "outputId": "58d8dc50-6c32-43ee-b5da-a3e7dec5f7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Output: tensor([[[[0.4294, 0.5629, 0.4286, 0.4140, 0.6244, 0.5207, 0.4923, 0.6437],\n",
            "          [0.4184, 0.5716, 0.4522, 0.4255, 0.6153, 0.5449, 0.4974, 0.6416],\n",
            "          [0.4178, 0.5583, 0.4482, 0.4362, 0.6213, 0.5589, 0.4865, 0.6394],\n",
            "          [0.4389, 0.5692, 0.4361, 0.4099, 0.6199, 0.5098, 0.5100, 0.6487],\n",
            "          [0.4336, 0.5425, 0.4261, 0.4334, 0.6340, 0.5364, 0.4855, 0.6445]],\n",
            "\n",
            "         [[0.4535, 0.2985, 0.6301, 0.5909, 0.3203, 0.3760, 0.4359, 0.4594],\n",
            "          [0.4283, 0.3025, 0.6410, 0.5801, 0.3096, 0.3835, 0.4203, 0.4774],\n",
            "          [0.4484, 0.2855, 0.6110, 0.5648, 0.3443, 0.3681, 0.4260, 0.4785],\n",
            "          [0.4219, 0.3068, 0.6253, 0.5858, 0.3203, 0.3987, 0.4320, 0.4801],\n",
            "          [0.4561, 0.2828, 0.6234, 0.5598, 0.3422, 0.3559, 0.4220, 0.4770]],\n",
            "\n",
            "         [[0.5969, 0.3473, 0.5983, 0.7760, 0.2266, 0.5947, 0.5390, 0.4170],\n",
            "          [0.5864, 0.3557, 0.6072, 0.7752, 0.2069, 0.5739, 0.5364, 0.3936],\n",
            "          [0.6045, 0.3578, 0.5809, 0.7697, 0.2355, 0.6118, 0.5193, 0.4170],\n",
            "          [0.5779, 0.3484, 0.6231, 0.7784, 0.2012, 0.5683, 0.5443, 0.3992],\n",
            "          [0.5866, 0.3353, 0.6193, 0.7842, 0.2163, 0.5713, 0.5601, 0.4136]],\n",
            "\n",
            "         [[0.7548, 0.7058, 0.5524, 0.5406, 0.3642, 0.4727, 0.4396, 0.4626],\n",
            "          [0.7576, 0.7026, 0.5550, 0.5238, 0.3695, 0.4635, 0.4311, 0.4697],\n",
            "          [0.7563, 0.7049, 0.5507, 0.5281, 0.3591, 0.4640, 0.4421, 0.4651],\n",
            "          [0.7498, 0.6937, 0.5548, 0.5192, 0.3489, 0.4563, 0.4660, 0.4780],\n",
            "          [0.7486, 0.6886, 0.5558, 0.5080, 0.3437, 0.4481, 0.4746, 0.4847]]],\n",
            "\n",
            "\n",
            "        [[[0.5211, 0.3598, 0.3547, 0.4911, 0.5811, 0.3513, 0.4473, 0.3045],\n",
            "          [0.5174, 0.3550, 0.3575, 0.4831, 0.5898, 0.3499, 0.4448, 0.3051],\n",
            "          [0.5160, 0.3215, 0.3747, 0.5378, 0.5492, 0.3947, 0.4179, 0.3006],\n",
            "          [0.5075, 0.3636, 0.3403, 0.4904, 0.5782, 0.3743, 0.4455, 0.2907],\n",
            "          [0.5122, 0.3739, 0.3338, 0.4978, 0.5692, 0.3818, 0.4469, 0.2882]],\n",
            "\n",
            "         [[0.5581, 0.3765, 0.4308, 0.3434, 0.3936, 0.5911, 0.6113, 0.4134],\n",
            "          [0.5886, 0.3921, 0.4160, 0.3554, 0.3854, 0.5896, 0.6023, 0.4026],\n",
            "          [0.5991, 0.4049, 0.4258, 0.3570, 0.3760, 0.5808, 0.6024, 0.4113],\n",
            "          [0.5815, 0.4019, 0.4459, 0.3474, 0.3739, 0.5762, 0.6075, 0.4267],\n",
            "          [0.6003, 0.4001, 0.4238, 0.3573, 0.3692, 0.5918, 0.6098, 0.4194]],\n",
            "\n",
            "         [[0.5300, 0.6454, 0.5330, 0.7011, 0.5153, 0.4450, 0.5308, 0.4059],\n",
            "          [0.5365, 0.6506, 0.5425, 0.6990, 0.5118, 0.4284, 0.5474, 0.4018],\n",
            "          [0.5414, 0.6609, 0.5385, 0.6936, 0.5144, 0.4353, 0.5257, 0.3892],\n",
            "          [0.5381, 0.6558, 0.5391, 0.6970, 0.5120, 0.4332, 0.5311, 0.3942],\n",
            "          [0.5185, 0.6275, 0.5327, 0.7113, 0.5126, 0.4406, 0.5524, 0.4266]],\n",
            "\n",
            "         [[0.6230, 0.7327, 0.4750, 0.4478, 0.5472, 0.7199, 0.3462, 0.5195],\n",
            "          [0.6189, 0.7617, 0.4714, 0.5102, 0.5536, 0.7516, 0.3398, 0.4800],\n",
            "          [0.6140, 0.7609, 0.4652, 0.4800, 0.5492, 0.7518, 0.3519, 0.4961],\n",
            "          [0.6304, 0.7528, 0.4682, 0.4650, 0.5721, 0.7333, 0.3566, 0.5151],\n",
            "          [0.6262, 0.7677, 0.4644, 0.4893, 0.5734, 0.7499, 0.3560, 0.4987]]]])\n",
            "Attention Weights: tensor([[[[0.1989, 0.1653, 0.2446, 0.1856, 0.2056],\n",
            "          [0.2569, 0.1540, 0.2102, 0.1944, 0.1845],\n",
            "          [0.2582, 0.1564, 0.1998, 0.1752, 0.2105],\n",
            "          [0.2009, 0.1608, 0.2391, 0.2152, 0.1841],\n",
            "          [0.2009, 0.1749, 0.2194, 0.1691, 0.2357]],\n",
            "\n",
            "         [[0.2025, 0.2085, 0.1854, 0.2085, 0.1951],\n",
            "          [0.2131, 0.1882, 0.1908, 0.1845, 0.2234],\n",
            "          [0.1956, 0.2049, 0.2311, 0.1922, 0.1763],\n",
            "          [0.2311, 0.1789, 0.2084, 0.1884, 0.1932],\n",
            "          [0.1838, 0.2208, 0.2206, 0.1855, 0.1892]],\n",
            "\n",
            "         [[0.2629, 0.1662, 0.1861, 0.2136, 0.1713],\n",
            "          [0.3028, 0.1667, 0.1804, 0.2192, 0.1309],\n",
            "          [0.2302, 0.1904, 0.2024, 0.2150, 0.1621],\n",
            "          [0.3152, 0.1437, 0.1741, 0.2286, 0.1384],\n",
            "          [0.3016, 0.1366, 0.1727, 0.2065, 0.1825]],\n",
            "\n",
            "         [[0.2149, 0.1904, 0.2208, 0.1642, 0.2096],\n",
            "          [0.2411, 0.1834, 0.1927, 0.1749, 0.2079],\n",
            "          [0.2260, 0.1889, 0.2091, 0.1559, 0.2201],\n",
            "          [0.2077, 0.1893, 0.1927, 0.1576, 0.2527],\n",
            "          [0.2096, 0.1875, 0.1776, 0.1559, 0.2694]]],\n",
            "\n",
            "\n",
            "        [[[0.1712, 0.2007, 0.2355, 0.2244, 0.1682],\n",
            "          [0.1830, 0.1928, 0.2301, 0.2218, 0.1724],\n",
            "          [0.1683, 0.2068, 0.2910, 0.1695, 0.1645],\n",
            "          [0.1807, 0.1931, 0.2558, 0.2295, 0.1409],\n",
            "          [0.1645, 0.1948, 0.2676, 0.2406, 0.1325]],\n",
            "\n",
            "         [[0.2338, 0.1612, 0.2204, 0.2054, 0.1792],\n",
            "          [0.2143, 0.1743, 0.1775, 0.2246, 0.2094],\n",
            "          [0.2057, 0.1961, 0.1645, 0.2216, 0.2121],\n",
            "          [0.2124, 0.2011, 0.1920, 0.2058, 0.1887],\n",
            "          [0.1911, 0.1908, 0.1820, 0.2067, 0.2294]],\n",
            "\n",
            "         [[0.2038, 0.1761, 0.2304, 0.2059, 0.1837],\n",
            "          [0.2202, 0.1643, 0.2568, 0.1937, 0.1650],\n",
            "          [0.2145, 0.1831, 0.2279, 0.2143, 0.1603],\n",
            "          [0.2152, 0.1836, 0.2331, 0.2015, 0.1666],\n",
            "          [0.1997, 0.1669, 0.2606, 0.1690, 0.2038]],\n",
            "\n",
            "         [[0.2144, 0.2092, 0.1620, 0.2597, 0.1548],\n",
            "          [0.1602, 0.2203, 0.1329, 0.2662, 0.2204],\n",
            "          [0.1674, 0.2115, 0.1558, 0.2775, 0.1878],\n",
            "          [0.1957, 0.1865, 0.1507, 0.2799, 0.1872],\n",
            "          [0.1693, 0.1903, 0.1425, 0.2854, 0.2126]]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    return (lambda d_k, scores, attention_weights: (torch.matmul(attention_weights, V), attention_weights))(\n",
        "        Q.shape[-1],\n",
        "        torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(Q.shape[-1], dtype=torch.float32)),\n",
        "        F.softmax(torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(Q.shape[-1], dtype=torch.float32)), dim=-1)\n",
        "    )\n",
        "\n",
        "# Example usage\n",
        "batch_size, num_heads, seq_len, d_k, d_v = 2, 4, 5, 8, 8\n",
        "Q, K, V = (torch.rand(batch_size, num_heads, seq_len, d) for d in (d_k, d_k, d_v))\n",
        "output, weights = scaled_dot_product_attention(Q, K, V)\n",
        "print(\"Attention Output:\", output)\n",
        "print(\"Attention Weights:\", weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWrUl579anzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}